"louisiana", "maine", "maryland", "massachusetts", "michigan",
"minnesota", "mississippi", "missouri", "montana", "nebraska", "nevada",
"new hampshire", "new jersey", "new mexico", "new york", "north carolina",
"north dakota", "ohio", "oklahoma", "oregon", "pennsylvania", "rhode island",
"south carolina", "south dakota", "tennessee", "texas", "utah", "vermont",
"virginia", "washington", "west virginia", "wisconsin", "wyoming")
)
# Join the datasets to convert FIPS to state names
data <- data %>%
left_join(fips_to_state, by = "statefip")
# Count the number of times each state appears in the dataset
state_counts <- data %>%
count(state, name = "frequency")
# Join the state counts with the map data
us_states_data <- us_states %>%
mutate(region = as.character(region)) %>%
left_join(state_counts, by = c("region" = "state"))
# Create the map
ggplot(us_states_data, aes(long, lat, group = group, fill = frequency)) +
geom_polygon(color = "white", size = 0.2) +
scale_fill_gradient(
low = "lightblue",
high = "darkblue",
na.value = "lightgray",
name = "Frequency"
) +
theme_void() +
labs(
title = "US States Shaded by Frequency in Dataset",
caption = "Data source: ACS"
)
# Select only numeric variables
numeric_data <- data %>% select_if(is.numeric)
# Summarize the statistics for numeric columns
summary_table <- data.frame(
Variable = colnames(numeric_data),
Mean = round(sapply(numeric_data, mean, na.rm = TRUE), 2),
Variance = round(sapply(numeric_data, var, na.rm = TRUE), 2),
Min = round(sapply(numeric_data, min, na.rm = TRUE), 2),
Max = round(sapply(numeric_data, max, na.rm = TRUE), 2)
)
rownames(summary_table) <- 1:nrow(summary_table)
# Print the table nicely
kable(summary_table, format = "markdown", caption = "Summary Statistics for Numeric Variables")
library(knitr)
library(tidyverse)
library(knitr)
# Mohamed
library(tidysynth)
# Alejandro
library(maps)
library(mapproj)
library(stargazer)
library(sandwich)
library(lmtest)
knitr::opts_chunk$set(out.width = '75%', out.height = '75%',
echo = FALSE, fig.align = 'center')
acs <- readRDS("acs_sample.rds")
# Not including taxincl, insincl and debt_incurred because first two only have zero
# and last one only has false
#year_from_start is perfectly correlated with year so we removed it.
data <- acs %>% select(year, density, metro, fips,
hhincome,
rooms, builtyr2, unitsstr, bedrooms,
age, married, male,
complete_plumbing, has_kitchen, statefip,
cpi_rent) %>%
na.omit() %>%
mutate(year = as.factor(year))
# Load the US states map data
us_states <- map_data("state")
# FIPS to state name mapping
fips_to_state <- data.frame(
statefip = c(01, 02, 04, 05, 06, 08, 09, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23,
24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,
42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56),
state = c("alabama", "alaska", "arizona", "arkansas", "california", "colorado",
"connecticut", "delaware", "district of columbia", "florida", "georgia",
"hawaii", "idaho", "illinois", "indiana", "iowa", "kansas", "kentucky",
"louisiana", "maine", "maryland", "massachusetts", "michigan",
"minnesota", "mississippi", "missouri", "montana", "nebraska", "nevada",
"new hampshire", "new jersey", "new mexico", "new york", "north carolina",
"north dakota", "ohio", "oklahoma", "oregon", "pennsylvania", "rhode island",
"south carolina", "south dakota", "tennessee", "texas", "utah", "vermont",
"virginia", "washington", "west virginia", "wisconsin", "wyoming")
)
# Join the datasets to convert FIPS to state names
data <- data %>%
left_join(fips_to_state, by = "statefip")
# Count the number of times each state appears in the dataset
state_counts <- data %>%
count(state, name = "frequency")
# Join the state counts with the map data
us_states_data <- us_states %>%
mutate(region = as.character(region)) %>%
left_join(state_counts, by = c("region" = "state"))
# Create the map
ggplot(us_states_data, aes(long, lat, group = group, fill = frequency)) +
geom_polygon(color = "white", size = 0.2) +
scale_fill_gradient(
low = "lightblue",
high = "darkblue",
na.value = "lightgray",
name = "Frequency"
) +
theme_void() +
labs(
title = "US States Shaded by Frequency in Dataset",
caption = "Data source: ACS"
)
# Select only numeric variables
numeric_data <- data %>% select_if(is.numeric)
# Summarize the statistics for numeric columns
summary_table <- data.frame(
Variable = colnames(numeric_data),
Mean = round(sapply(numeric_data, mean, na.rm = TRUE), 2),
Variance = round(sapply(numeric_data, var, na.rm = TRUE), 2),
Min = round(sapply(numeric_data, min, na.rm = TRUE), 2),
Max = round(sapply(numeric_data, max, na.rm = TRUE), 2)
)
rownames(summary_table) <- 1:nrow(summary_table)
# Print the table nicely
kable(summary_table, format = "markdown", caption = "Summary Statistics for Numeric Variables")
# Select only factor (categorical) variables
factor_data <- data %>% select(-fips) %>%
select_if(is.factor)
# Summarize the statistics for factor columns
summary_table2 <- data.frame(
Variable = colnames(factor_data),
`Number of Levels` = sapply(factor_data, function(col) length(unique(col))),
`Most Frequent Level` = sapply(factor_data, function(col) names(sort(table(col), decreasing = TRUE)[1])),
`Frequency of Most Frequent Level` = sapply(factor_data, function(col) max(table(col)))
)
# Print the table nicely
kable(summary_table2, format = "markdown", caption = "Summary Statistics for Factor Variables")
data <- data %>% select(-fips)
fit.lm_acs <- lm(cpi_rent ~ ., data)
summary(fit.lm_acs)
coeftest(fit.lm_acs, vcov=vcovHC, type="HC0")
vif <- vif(fit.lm_acs)
?vif
??vif
library(tidyverse)
library(knitr)
# Mohamed
library(tidysynth)
# Alejandro
library(maps)
library(mapproj)
library(stargazer)
library(sandwich)
library(lmtest)
library(car)
knitr::opts_chunk$set(out.width = '75%', out.height = '75%',
echo = FALSE, fig.align = 'center')
acs <- readRDS("acs_sample.rds")
# Not including taxincl, insincl and debt_incurred because first two only have zero
# and last one only has false
#year_from_start is perfectly correlated with year so we removed it.
data <- acs %>% select(year, density, metro, fips,
hhincome,
rooms, builtyr2, unitsstr, bedrooms,
age, married, male,
complete_plumbing, has_kitchen, statefip,
cpi_rent) %>%
na.omit() %>%
mutate(year = as.factor(year))
# Load the US states map data
us_states <- map_data("state")
# FIPS to state name mapping
fips_to_state <- data.frame(
statefip = c(01, 02, 04, 05, 06, 08, 09, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23,
24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,
42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56),
state = c("alabama", "alaska", "arizona", "arkansas", "california", "colorado",
"connecticut", "delaware", "district of columbia", "florida", "georgia",
"hawaii", "idaho", "illinois", "indiana", "iowa", "kansas", "kentucky",
"louisiana", "maine", "maryland", "massachusetts", "michigan",
"minnesota", "mississippi", "missouri", "montana", "nebraska", "nevada",
"new hampshire", "new jersey", "new mexico", "new york", "north carolina",
"north dakota", "ohio", "oklahoma", "oregon", "pennsylvania", "rhode island",
"south carolina", "south dakota", "tennessee", "texas", "utah", "vermont",
"virginia", "washington", "west virginia", "wisconsin", "wyoming")
)
# Join the datasets to convert FIPS to state names
data <- data %>%
left_join(fips_to_state, by = "statefip")
# Count the number of times each state appears in the dataset
state_counts <- data %>%
count(state, name = "frequency")
# Join the state counts with the map data
us_states_data <- us_states %>%
mutate(region = as.character(region)) %>%
left_join(state_counts, by = c("region" = "state"))
# Create the map
ggplot(us_states_data, aes(long, lat, group = group, fill = frequency)) +
geom_polygon(color = "white", size = 0.2) +
scale_fill_gradient(
low = "lightblue",
high = "darkblue",
na.value = "lightgray",
name = "Frequency"
) +
theme_void() +
labs(
title = "US States Shaded by Frequency in Dataset",
caption = "Data source: ACS"
)
# Select only numeric variables
numeric_data <- data %>% select_if(is.numeric)
# Summarize the statistics for numeric columns
summary_table <- data.frame(
Variable = colnames(numeric_data),
Mean = round(sapply(numeric_data, mean, na.rm = TRUE), 2),
Variance = round(sapply(numeric_data, var, na.rm = TRUE), 2),
Min = round(sapply(numeric_data, min, na.rm = TRUE), 2),
Max = round(sapply(numeric_data, max, na.rm = TRUE), 2)
)
rownames(summary_table) <- 1:nrow(summary_table)
# Print the table nicely
kable(summary_table, format = "markdown", caption = "Summary Statistics for Numeric Variables")
# Select only factor (categorical) variables
factor_data <- data %>% select(-fips) %>%
select_if(is.factor)
# Summarize the statistics for factor columns
summary_table2 <- data.frame(
Variable = colnames(factor_data),
`Number of Levels` = sapply(factor_data, function(col) length(unique(col))),
`Most Frequent Level` = sapply(factor_data, function(col) names(sort(table(col), decreasing = TRUE)[1])),
`Frequency of Most Frequent Level` = sapply(factor_data, function(col) max(table(col)))
)
# Print the table nicely
kable(summary_table2, format = "markdown", caption = "Summary Statistics for Factor Variables")
data <- data %>% select(-fips)
fit.lm_acs <- lm(cpi_rent ~ ., data)
summary(fit.lm_acs)
coeftest(fit.lm_acs, vcov=vcovHC, type="HC0")
vif <- vif(fit.lm_acs)
?vif
library(tidyverse)
library(knitr)
# Mohamed
library(tidysynth)
# Alejandro
library(maps)
library(mapproj)
library(stargazer)
library(sandwich)
library(lmtest)
library(car)
knitr::opts_chunk$set(out.width = '75%', out.height = '75%',
echo = FALSE, fig.align = 'center')
acs <- readRDS("acs_sample.rds")
# Not including taxincl, insincl and debt_incurred because first two only have zero
# and last one only has false
#year_from_start is perfectly correlated with year so we removed it.
data <- acs %>% select(year, density, metro, fips,
hhincome,
rooms, builtyr2, unitsstr, bedrooms,
age, married, male,
complete_plumbing, has_kitchen, statefip,
cpi_rent) %>%
na.omit() %>%
mutate(year = as.factor(year))
# Load the US states map data
us_states <- map_data("state")
# FIPS to state name mapping
fips_to_state <- data.frame(
statefip = c(01, 02, 04, 05, 06, 08, 09, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23,
24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,
42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56),
state = c("alabama", "alaska", "arizona", "arkansas", "california", "colorado",
"connecticut", "delaware", "district of columbia", "florida", "georgia",
"hawaii", "idaho", "illinois", "indiana", "iowa", "kansas", "kentucky",
"louisiana", "maine", "maryland", "massachusetts", "michigan",
"minnesota", "mississippi", "missouri", "montana", "nebraska", "nevada",
"new hampshire", "new jersey", "new mexico", "new york", "north carolina",
"north dakota", "ohio", "oklahoma", "oregon", "pennsylvania", "rhode island",
"south carolina", "south dakota", "tennessee", "texas", "utah", "vermont",
"virginia", "washington", "west virginia", "wisconsin", "wyoming")
)
# Join the datasets to convert FIPS to state names
data <- data %>%
left_join(fips_to_state, by = "statefip")
# Count the number of times each state appears in the dataset
state_counts <- data %>%
count(state, name = "frequency")
# Join the state counts with the map data
us_states_data <- us_states %>%
mutate(region = as.character(region)) %>%
left_join(state_counts, by = c("region" = "state"))
# Create the map
ggplot(us_states_data, aes(long, lat, group = group, fill = frequency)) +
geom_polygon(color = "white", size = 0.2) +
scale_fill_gradient(
low = "lightblue",
high = "darkblue",
na.value = "lightgray",
name = "Frequency"
) +
theme_void() +
labs(
title = "US States Shaded by Frequency in Dataset",
caption = "Data source: ACS"
)
# Select only numeric variables
numeric_data <- data %>% select_if(is.numeric)
# Summarize the statistics for numeric columns
summary_table <- data.frame(
Variable = colnames(numeric_data),
Mean = round(sapply(numeric_data, mean, na.rm = TRUE), 2),
Variance = round(sapply(numeric_data, var, na.rm = TRUE), 2),
Min = round(sapply(numeric_data, min, na.rm = TRUE), 2),
Max = round(sapply(numeric_data, max, na.rm = TRUE), 2)
)
rownames(summary_table) <- 1:nrow(summary_table)
# Print the table nicely
kable(summary_table, format = "markdown", caption = "Summary Statistics for Numeric Variables")
# Select only factor (categorical) variables
factor_data <- data %>% select(-fips) %>%
select_if(is.factor)
# Summarize the statistics for factor columns
summary_table2 <- data.frame(
Variable = colnames(factor_data),
`Number of Levels` = sapply(factor_data, function(col) length(unique(col))),
`Most Frequent Level` = sapply(factor_data, function(col) names(sort(table(col), decreasing = TRUE)[1])),
`Frequency of Most Frequent Level` = sapply(factor_data, function(col) max(table(col)))
)
# Print the table nicely
kable(summary_table2, format = "markdown", caption = "Summary Statistics for Factor Variables")
data <- data %>% select(-fips)
fit.lm_acs <- lm(cpi_rent ~ ., data)
summary(fit.lm_acs)
coeftest(fit.lm_acs, vcov=vcovHC, type="HC0")
#vif <- vif(fit.lm_acs)
print(vif)
?stargazer
stargazer(fit.lm_acs)
stargazer(fit.lm_acs, "text")
stargazer(fit.lm_acs, type = "text")
library(tidyverse)
library(tidysynth)
knitr::opts_chunk$set(out.width = '75%', out.height = '75%',
echo = FALSE, fig.align = 'center')
constr = read_csv("CausalAnalysis/Final/ConstructionPriceIndex/CONSTR.csv")
ggplot2::theme_set(
theme(text = element_text(size = 16),
axis.text = element_text(size = 12))
)
plotConstrVars = function(vars, after = 2012){
constr2 = constr %>%
filter(year >= after) %>%
pivot_longer(all_of(vars))
breaks = (constr2$date %>% unique)[seq(1, length(constr$date), by = 6)]
ggplot(constr2) +
geom_line(aes(x = date, y = value)) +
facet_wrap(name ~ ., ncol = 1, scales = "free_y") +
scale_x_continuous(breaks = breaks,
guide = guide_axis(angle = 45))
}
plotConstrVars("sold_corrected", 0) +
theme(axis.text.x = element_text(angle = 45, hjust = 0))
plotConstrVars(c("sold_corrected", "cpi_mspus"))
plotConstrVars(c("cpi_mspus", "cpi_sold"))
plotConstrVars("sold_corrected_cash")
plotConstrVars(c("constr_corrected", "sold_corrected"))
fmr_synth = read_csv("CausalAnalysis/Final/Minnesota/FMR_synth.csv")
treat_date = 2020
treated_unit = "Minneapolis-St. Paul-Bloomington, MN-WI HUD Metro FMR Area"
synth = fmr_synth %>%
synthetic_control(outcome = price,
unit = areaname22,
time = year,
i_unit = treated_unit,
i_time = treat_date) %>%
generate_predictor(time_window = 2016,
populat = pop2010,
mdom = median_days_on_market,
nlc = new_listing_count,
plc = pending_listing_count,
pic = price_increased_count,
msf = median_square_feet
) %>%
generate_weights() %>%
generate_control()
plot_trends(synth)
plot_differences(synth)
?stargazer
stargazer(fit.lm_acs, type = "text", omit = "year")
stargazer(fit.lm_acs, type = "text", omit = c("year", "state"))
coeftest(fit.lm_acs, vcov=vcovHC, type="HC0") %>% stargazer
data <- data %>% select(-fips)
fit.lm_acs <- lm(cpi_rent ~ ., data)
fit.coeftest = coeftest(fit.lm_acs, vcov=vcovHC, type="HC0")
stargazer(fit.coeftest, type = "text", omit = c("year", "state"))
coeftest(fit.lm_acs, vcov=vcovHC, type="HC0")
?plm
??plm
?stargazer
longtable
??longtable
?kable
?stargazer
theme
# Set tigris options for retrieving data
options(tigris_use_cache = TRUE)
# Set tigris options
options(tigris_use_cache = TRUE)
# Fetch PUMA shapefiles for the required states
# Use a loop or lapply to fetch PUMAs for all statefips in your dataset
puma_shapefiles <- lapply(unique(data$statefip), function(state) {
pumas(state = state, cb = TRUE, year = 2020) # Download PUMAs for the given state
})
# Combine all state shapefiles into a single sf object
puma_combined <- do.call(rbind, puma_shapefiles)
# Filter PUMAs based on the dataset
highlighted_pumas <- puma_combined %>%
filter(as.integer(STATEFP20) %in% data$statefip & as.integer(PUMACE20) %in% data$puma)
map_plot <- ggplot() +
geom_sf(data = puma_combined, fill = "lightgray", color = "white", size = 0.2) + # Background PUMAs
geom_sf(data = highlighted_pumas, fill = "red", color = "black", size = 0.4) +  # Highlighted PUMAs
coord_sf(
xlim = c(-130, -60), # Longitude limits for continental U.S.
ylim = c(20, 55)     # Latitude limits for continental U.S.
) +
theme_minimal() +
labs(title = "Highlighted PUMAs",
caption = "Source: U.S. Census Bureau") +
theme(
plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
plot.caption = element_text(size = 12),
axis.text = element_blank(),  # Remove axis labels for clarity
axis.ticks = element_blank()
)
map_plot
acs %>% filter(year == 2019)
data
data %>% filter(year == 2019)
highlighted_pumas
puma_combined
data$puma
acs$puma
data <- acs %>% select(year, density, metro, fips,
hhincome, puma,
rooms, builtyr2, unitsstr, bedrooms,
age, married, male,
complete_plumbing, has_kitchen, statefip,
cpi_rent) %>%
na.omit() %>%
mutate(year = as.factor(year))
# Set tigris options for retrieving data
options(tigris_use_cache = TRUE)
# Set tigris options
options(tigris_use_cache = TRUE)
# Fetch PUMA shapefiles for the required states
# Use a loop or lapply to fetch PUMAs for all statefips in your dataset
puma_shapefiles <- lapply(unique(data$statefip), function(state) {
pumas(state = state, cb = TRUE, year = 2020) # Download PUMAs for the given state
})
# Combine all state shapefiles into a single sf object
puma_combined <- do.call(rbind, puma_shapefiles)
# Filter PUMAs based on the dataset
highlighted_pumas <- puma_combined %>%
filter(as.integer(STATEFP20) %in% data$statefip & as.integer(PUMACE20) %in% data$puma)
map_plot <- ggplot() +
geom_sf(data = puma_combined, fill = "lightgray", color = "white", size = 0.2) + # Background PUMAs
geom_sf(data = highlighted_pumas, fill = "red", color = "black", size = 0.4) +  # Highlighted PUMAs
coord_sf(
xlim = c(-130, -60), # Longitude limits for continental U.S.
ylim = c(20, 55)     # Latitude limits for continental U.S.
) +
theme_minimal() +
labs(title = "Highlighted PUMAs",
caption = "Source: U.S. Census Bureau") +
theme(
plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
plot.caption = element_text(size = 12),
axis.text = element_blank(),  # Remove axis labels for clarity
axis.ticks = element_blank()
)
map_plot
# Set tigris options for retrieving data
options(tigris_use_cache = TRUE)
# Set tigris options
options(tigris_use_cache = TRUE)
# Fetch PUMA shapefiles for the required states
# Use a loop or lapply to fetch PUMAs for all statefips in your dataset
puma_shapefiles <- lapply(unique(data$statefip), function(state) {
pumas(state = state, cb = TRUE, year = 2020) # Download PUMAs for the given state
})
# Combine all state shapefiles into a single sf object
puma_combined <- do.call(rbind, puma_shapefiles)
# Filter PUMAs based on the dataset
highlighted_pumas <- puma_combined %>%
filter(as.integer(STATEFP20) %in% data$statefip & as.integer(PUMACE20) %in% data$puma)
map_plot <- ggplot() +
geom_sf(data = puma_combined, fill = "lightgray", color = "white", size = 0.2) + # Background PUMAs
geom_sf(data = highlighted_pumas, fill = "red", color = "black", size = 0.4) +  # Highlighted PUMAs
coord_sf(
xlim = c(-130, -60), # Longitude limits for continental U.S.
ylim = c(20, 55)     # Latitude limits for continental U.S.
) +
theme_minimal() +
labs(title = "Highlighted PUMAs",
caption = "Source: U.S. Census Bureau") +
theme(
plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
plot.caption = element_text(size = 12),
axis.text = element_blank(),  # Remove axis labels for clarity
axis.ticks = element_blank()
)
map_plot
